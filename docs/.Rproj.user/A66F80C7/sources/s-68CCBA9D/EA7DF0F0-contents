---
output:
  html_document: default
  pdf_document: default
  word_document: default
---
# Water Quality Statistics  

```{r knitrinitmcmc1, echo=FALSE,  message=FALSE, warning=FALSE, cache=FALSE}
library(knitr)
library(rmdformats)
library(hrbrthemes)
library(tidyverse)
library(showtext)
library(kableExtra)
library(MCMCglmm)
library(bayesplot)
library(stargazer)
font_add("Roboto Condensed","robotocondensed-regular.ttf")


### Global options

options(warning = FALSE,message = FALSE,echo=FALSE,cache=TRUE)
opts_chunk$set(echo=FALSE,
               message=FALSE,
                warning=FALSE,cache=TRUE)
opts_knit$set(warning = FALSE,message = FALSE,echo=FALSE,cache=TRUE)

## ggploting theme
#theme_set(hrbrthemes::theme_ipsum_rc())
#hrbrthemes::import_roboto_condensed()
#hrbrthemes::update_geom_font_defaults()
```
We developed a spatial regression model to estimate concentrations for constituents of concern (COCs) in Puget Sound urban stormwater. We first used a linear mixed model to select spatial regression parameters. We then used a censored Markov chain Monte Carlo simulation to perform final regressions. 

## Data Sources 


```{r knitr_init, cache=FALSE, include=FALSE}
#rm(list = ls())
#set the working directory 
#setwd("~/repos/stormwaterheatmap/R-scripts/WatershedRegression")

#load packages
library(knitr)
library(tidyverse)
library(car)
library(caret)
library(psych)
library(DataExplorer)
library(dplyr)
library(readr)
library(lme4)
library(nlme)
library(hrbrthemes)
library(sjPlot)
library(Metrics)



## Global options

opts_chunk$set(prompt=FALSE,
               message=FALSE,
               warning=FALSE)
options(scipen = 1, digits = 3)

#set seed for reproducibility 
set.seed(50)
```



### Outfall Data

The primary source of measured stormwater data is the S8.D Municipal Stormwater Permit Outfall Data (referred to as the S8 Data in this document) provided by the Washington Department of Ecology [@No2015]. Special Condition S8.D of the 2007-2012 Phase I Municipal Stormwater Permit required permittees to collect and analyze data to evaluate pollutant loadings of stormwater discharged from different land uses: high density (HD) residential, low density (LD) residential, commercial, and industrial. Phase I Permittees^[Cities of Tacoma and Seattle; King, Snohomish, Pierce and Clark counties; Ports of Tacoma and Seattle] collected water quality and flow data, sediment data, and toxicity information from stormwater discharges during storm events.

The stormwater outfall data is available from Ecology via an open-data api at: https://data.wa.gov/Natural-Resources-Environment/Municipal-Stormwater-Permit-Outfall-Data/d958-q2ci.



```{r echo=FALSE, message=FALSE, warning=FALSE}
all.S8.data <- read.csv("data/S8_data.csv",
                        stringsAsFactors = FALSE )

#filter out rejected data
all.S8.data <- (filter(all.S8.data,!result_data_qualifier %in% 'REJ'))

#filter out replicates 
all.S8.data <- (filter(all.S8.data,!sample_replicate_flag %in% 'Y'))

#change nondetect warnings to detects
warnings <- all.S8.data$nondetect_flag == "WARNING"
all.S8.data$nondetect_flag[warnings] <- FALSE 

#Change NA to detect
all.S8.data$nondetect_flag[is.na(all.S8.data$nondetect_flag)] <- FALSE

#Change season to factor 
all.S8.data$season <- as.factor(all.S8.data$season)


```

COCs analyzed in this study are show in Table \@ref(tab:selparms). 

```{r selparms, fig.cap = "cocsofConcern"}
#Select Parameters
params <- c('Zinc - Water - Total',
 'Copper - Water - Total',
 'Nitrite-Nitrate - Water - Dissolved',
 'Lead - Water - Total',
 'Total Phosphorus - Water - Total',
 'Total Suspended Solids - Water - Total',
 'Total Phthalate - Water - Total',
'Total PAH - Water - Total',
#'Chrysene - Water - Total',
'CPAH - Water - Total',
'HPAH - Water - Total' 
#'Total Kjeldahl Nitrogen - Water - Total',
#'Total PCB - Water - Total'
)

#save a list of all the parameters in case we want to use mor. 
params.all <- data.frame(unique(all.S8.data$parameter))
s8data <- all.S8.data 
kable(params,col.names = "Constituent",caption = "Constituents of Concern Analyzed in this Study")
#
```

We extracted data for these COCs, and performed minimal data cleaning. We filtered out rejected data (values with a `R` or `REJ` flag), removed replicates, and removed three data points that were obvious outliers. While our analysis is not overly sensitive to outliers, three parameters had reported data that were orders of magnitude higher than the rest of the data. One high-outlier value was removed for each of the following COCs: Total Suspended Solids, Nitrite-Nitrate, and Total Phosphorus.  




```{r dataClean,  message=FALSE, warning=FALSE, include=FALSE}
s8data <- all.S8.data %>% 

  dplyr::select(
    study_name,
    location_id,parameter,
    type,
    season,
    new_result_value,
    nondetect_flag,
    study_id,
    access_id,
    field_collection_end_date,
    field_collection_start_date,
    type)


#rename some columns
colnames(s8data)[colnames(s8data) == "location_id"] <- "Location"
colnames(s8data)[colnames(s8data) == "new_result_value"] <-
  "concentration"
s8data$nondetect_flag <- as.logical(s8data$nondetect_flag)
s8data$concentration <- as.numeric(s8data$concentration)


```



Outliers were removed for Figure  \@ref(fig:outlier1) shows data before outliers were removed. Figure  \@ref(fig:outlier2) shows data after outliers were removed. 
  
```{r outlier1, fig.cap = "All observations - outliers in place"}
#make a function for scatter plots 
scatter_cocs <- function(df.coc,title) {
 p <- ggplot(df.coc, aes(1, concentration)) + geom_jitter() + labs(
  title = title,
  subtitle = "Data collected 2009-2013",
  caption =
    " Data source: Ecology, 2015",
  x = "Observations"
)
p + facet_wrap( ~ parameter, scales = 'free')+theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) 
}

scatter_cocs(s8data[which(s8data$parameter %in% params),],'All Observations')
```
   
Quantile-quantile plots of COCs analyzed are shown in \@ref(fig:tidyqqplots) 

```{r tidyqqplots, fig.cap = "Quantile-quantile plots of COCs, log scale"}
s8subset <- s8data[which(s8data$parameter %in% params), ]
s8subset$log_concentration <- log(s8subset$concentration)
ggplot(s8subset)+stat_qq(aes(sample=concentration))+
stat_qq_line(aes(sample=concentration))+facet_wrap(ncol = 3, ~parameter,scales = "free_y")+scale_y_log10()+ggtitle("QQ diagnostic plots - all COCs")
```



```{r outlier2,  fig.cap = "All observations - outliers removed"}
#remove and replot 

outlierParams <- c("Total Suspended Solids - Water - Total", "Total Phosphorus - Water - Total", "Nitrite-Nitrate - Water - Dissolved")

#This removes the highest values 
outlierVals <-
  top_n(group_by(s8data[which(s8data$parameter %in% outlierParams), ], parameter), 1, concentration)$concentration

s8data <- s8data %>%
  group_by(parameter) %>%
  slice(which(!(
    parameter %in% outlierParams & concentration %in% outlierVals
  )))

scatter_cocs(s8data[which(s8data$parameter %in% params),],'All Observations - Outliers Removed')

```


## Spatial data 

For this study, we did not rely on the permittee’s self-reported land use type to run regression models predicting pollution loading from land use.  A visual scan of our land cover data layer versus self-reported land use types revealed little agreement among permittee definitions of the four land use types (high density residential, low density residential, commercial, industrial).  Therefore, we compiled a suite of continuous and categorical landscape datasets from which to run prediction loading models.  We divide these into land use and landscape data. 

### Land use 

In order to employ a consistent analysis across different monitored watersheds we extracted land use data  from Ecology’s 2010 Statewide Land use data set^[See: https://fortress.wa.gov/ecy/gispublic/DataDownload/ECY_CAD_Landuse2010.htm for more information]. Ecology generated the coverage from digital county tax parcel layers using Department of Revenue (DOR) two digit land use codes (see; WAC 458-53-030, Stratification of assessment rolls - real property).

### Landscape data  

For each watershed contained in the S8 dataset, potentially relevant landscape data was extracted from the following sources below: 

| Layer                   | ID          | Source |
|-------------------------|-------------|---------| 
|   Nighttime Lights      |  nighttime_lights | [Global Radiance Calibrated Nighttime Lights](https://doi.org/10.1117/12.2023107)    
|  Particulate Matter 2.5μm |  pm25      |   [van Donkelaar et al. 2018.](https://doi.org/10.7927/H4ZK5DQS)   
|    Rooftop Density      |    roofs   |     [Microsoft AI US Building Footprints](https://github.com/microsoft/USBuildingFootprints)   |     1  |
| Imperviousness          | impervious      | TNC Puget Sound land cover
|Age of Impervious Surface|change_year_index |[Tsinghua University FROM-GLC year of change to impervious surface](http://doi.org/10.1016/j.rse.2019.111510)
|Logarithm of Population Density |logPopulation |[CIESIN - Columbia University Gridded Population of the World, Version 4](https://doi.org/10.7927/H49C6VHW)
|Logarithm of Average Daily Traffic Volume  |logTraffic|[INRIX Traffic](https://inrix.com/products/volume)

## Methods

### Pre-processing of spatial data 
In order to use the landscape data at an appropriate scale across the study area, spatial predictors were stacked and then convolved with a 100-meter gaussian kernel. This resulted in a "fuzzy" set of predictors that could apply across dataset boundaries. These values were then extracted for each monitored watershed. Values were scaled and centered for regression purposes. 

```{r}
#Spatial predcitors have been extracted and saved as a csv file. 
#spatial_data<-read_csv("data/spatialPredictors_4_13.csv", col_types = cols(X1 = col_skip()))
spatial_data <- read_csv("data/spatialPredictors_521.csv")
#RES and COM are compositional data. Change to a ratio
spatial_data$LU_ratio = spatial_data$COM/spatial_data$RES 
spatial_data <- dplyr::select(spatial_data, -c(RES,COM,`system:index`))
#merge spatial predictors with monitoring data 
s8data.wPredictors <<- merge(s8data, spatial_data)%>% 
  dplyr::select(-c(depSplusN))

```




```{r functions}
getBaseFormula <- function(df.coc) {
  #Function to make a formula for prediction
  predictors <- df.coc %>%
    select_if(is.numeric) %>%
    dplyr::select(-c(concentration, access_id)) %>%
    colnames()
  return(as.formula(paste(
    "(concentration) ~",  (paste((predictors),  collapse = " + ")), " + (1|Location)"
  )))
  
}
```

 
```{r}
plot_s8 <- function(coc) {
    df.coc <- (base::subset(s8data.wPredictors,
                parameter == coc))
  #plot the data to inspect 
  plot <- ggplot(data=(df.coc))+geom_point(aes(x=Location,y=concentration))+scale_y_log10()+labs(
      y = "Concentration (µg/L)",
      x = "Location",
      title =  "Measured Concentrations",
      subtitle = coc ) 
  return(plot)
}
```



<!-- {r} -->
<!-- # #Calculate variance-inflation factors.  -->
<!-- #  -->
<!-- # calc_vif_base <- function(coc) { -->
<!-- #   df.coc <- (base::subset(s8data.wPredictors, -->
<!-- #                 parameter == coc)) -->
<!-- #   base_formula <- getBaseFormula(df.coc)#returns a formula with all predictors -->
<!-- #   model.1 <<- lmer(base_formula, data = df.coc, na.action = na.omit) -->
<!-- #   v <- sort(vif(model.1),decreasing=TRUE) -->
<!-- #   return(v) -->
<!-- #   #kable(v,caption="Variance Inflation Factors - not filtered") #display the vif of the dataset -->
<!-- # } -->

<!-- ``` -->


<!-- ## Function to iteratively remove mulitcolinear predictors  -->
<!-- Same as above, expect multicolinear predictors are removed.  -->
```{r}
## *Check VIF
 
check_vif <- function(coc) {
  df.coc <- (base::subset(s8data.wPredictors,
                parameter == coc))
  base_formula <- getBaseFormula(df.coc)#returns a formula with all predictors
  model.1 <- lmer(base_formula, data = df.coc, na.action = na.omit) #make into a lmer object 
  v <- sort(vif(model.1),decreasing=TRUE)
  
  #if the VIF of the highest ranked predictor is >10 then iteratively remove
  model_object <- model.1 #start with model object as the base model (all predictors included)
  
  
  for (i in 1:20) {
    interim_v <- sort(vif(model_object), decreasing = TRUE)
    if (max(interim_v) < 10) {
      break
    }
    predictor_to_drop = as.name(names(interim_v)[which(interim_v == max(interim_v))])
    model_object <-
      stats::update(model_object, paste(".~ . -", predictor_to_drop))
    }
  
  m1Terms <- (labels(terms(model.1)))
  m2Terms <- labels(terms(model_object))
  
  #compare the terms to get a list of the dropped terms
  droppedTerms <- setdiff(m1Terms, m2Terms)
  
  #make a list of selected predictors
  predictors <- m2Terms#colnames(model.frame(model_object)) 

  
  #filter df.coc to remove dropped terms.
  df.coc = dplyr::select(df.coc, -(droppedTerms))
  return(list("vif" = interim_v,"dropped" = droppedTerms,"predictors" = predictors))
  #kable(droppedTerms,caption = "These terms were dropped")
  
  #kable(interim_v,caption="Variance Inflation Factors - multicolinear factors dropped")
}

```

<!-- ## Function to perform stepwise selection and return a series of best models  -->

```{r stepwise, cache=TRUE}
## Stepwise Selection

#forward_selection(TRUE,coc,model_info$predictors)

# Extract the model that step found:

#perform forward selection on model parameters. First for non-transformed data, then for log-transformed data
forward_selection <- function(seasonal.bin, coc,predictors) {
  #seasonal.bin = binary (T/F) if seasonal model should be used
  library(lmerTest)
  #make this a lmer object 
  df.coc <- (base::subset(s8data.wPredictors,
                parameter == coc)) 
  model_object_formula <- as.formula(paste(
    "concentration ~",  (paste((predictors),  collapse = " + ")), " + (1|Location)"))
  
  model_object <- lmer(model_object_formula,data=df.coc)
  
  step.2 <-  lmerTest::step(model_object,reduce.random=FALSE,data=df.coc)
  step.2.log <- lmerTest::step(stats::update(model_object, log(concentration)~.))
  
  #extract the models 
  model.3 <- get_model(step.2)
  model.3.log <- get_model(step.2.log)
  
  #perform forward selection on model parameters, this time add seasonality . First for non-transformed data, then for log-transformed data
  step.4 <- lmerTest::step(stats::update(model_object,.~.+season))
  step.4.log <- lmerTest::step(stats::update(model_object,log(concentration) ~.+season))
  model.4 <- get_model(step.4)
  model.4.log <- get_model(step.4.log)
  
  #get formulas 
  model.3.formula <- as.formula(model.3@call$formula)
  model.3.log.formula <- as.formula(model.3.log@call$formula)
  model.4.formula <- as.formula(model.4@call$formula)
  model.4.log.formula <- as.formula(model.4.log@call$formula)
  
  #detach lmer test and remove the models. Keep the formulas. 
  detach("package:lmerTest", unload=TRUE)
  rm(model.3,model.3.log,model.4,model.4.log)
  
  #use lmer for performing modeling
  #calculate base model 
  df.coc <- (base::subset(s8data.wPredictors,
                parameter == coc)) 
  model.base <- lmer(model_object_formula,data=df.coc)
  model <- lmer(model.3.formula,data=df.coc)
  log_model<-  lmer(model.3.log.formula,data=df.coc)
  
  if(seasonal.bin) {
      #if seasonal model switch is on, calc seasonal models
    model_with_seasonality <- lmer(model.4.formula,data=df.coc)
    model_with_seasonality_log <- lmer(model.4.log.formula,data=df.coc)
      #add to list 
    modelList <- c(model,log_model,model_with_seasonality,model_with_seasonality_log)
    
    }
    else {
    modelList <-c(model,log_model)
    modelLables <-c('linear','log-linear') 
    }
   

 
 return(modelList)#,show.aic = TRUE)# = FALSE, title=coc, dv.labels = modelLables) 
  # #make a table of coefficients 
  # tab_model(modelList,
  #   model_log,
  #   model_with_seasonality,
  #   model.with_seasonality_log,
  #   show.aic = TRUE,auto.label = FALSE, title=coc, dv.labels = c('linear','log-linear','linear seasonal','log-linear seasonal'))#file=paste0("results/",coc,".html"))
}
```



```{r}

results = c()
plots = c()
vifs = c()
tabs = c()

for (i in 1:length(params)){
coc = params[i]
df.coc <- (base::subset(s8data.wPredictors,
                parameter == coc)) 
plots[[coc]] <- plot_s8(coc)
model_info <- check_vif(coc)
vifs[[coc]] <- model_info$vif

results[[coc]] = forward_selection(TRUE,coc,model_info$predictors)}
modelLabels <-c('linear','log-linear','linear seasonal','log-linear seasonal') 

```


```{r}
#Wrapper function for displaying results for individual cocs. 

show_resultsFun <- function(j){
#get parameter label 
lab = params[j]
models <- results[[j]]
  #print((summary(models[[k]])))
  #plot(models[[k]],,main=paste(lab,"\n","Resididuals"))
 (qqmath(models[[2]],main=paste(lab,"\n",modelLabels[2],"\n","QQ plot of resididuals")))
}

```



### Controlling for multicolinearity 

To address multicolinearity, we calculate the variance inflation factor (VIF) and iteratively remove parameters with the highest VIF. We keep removing parameters one at a time until all VIF values are below 10.0. Table \@ref(fig:vifTable) shows final VIF factors for Zinc. Similar values were found for all other COCs and are not reported here. 
```{r vifTable, echo=FALSE, message=FALSE, warning=FALSE}
j=1
  lab=params[j]
  kable(vifs[j],
      caption = paste("Variance inflation factors",lab),col.names = c("vif"))
# j=2
#   lab=params[j]
#   kable(vifs[j],
#       caption = paste("Variance inflation factors",lab),col.names = c("vif"))
# j=3
#   lab=params[j]
#   kable(vifs[j],
#       caption = paste("Variance inflation factors",lab),col.names = c("vif"))
# j=4
#   lab=params[j]
#   kable(vifs[j],
#       caption = paste("Variance inflation factors",lab),col.names = c("vif"))
# j=5
#   lab=params[j]
#   kable(vifs[j],
#       caption = paste("Variance inflation factors",lab),col.names = c("vif"))
# j=6
#   lab=params[j]
#   kable(vifs[j],
#       caption = paste("Variance inflation factors",lab),col.names = c("vif"))
# j=7
#   lab=params[j]
#   kable(vifs[j],
#       caption = paste("Variance inflation factors",lab),col.names = c("vif"))
# j=8
#   lab=params[j]
#   kable(vifs[j],
#       caption = paste("Variance inflation factors",lab),col.names = c("vif"))
# j=9
#   
# lab=params[j]
#   kable(vifs[j],
#       caption = paste("Variance inflation factors",lab),col.names = c("vif"))
#   
# j=10
#   lab=params[j]
#   kable(vifs[j],
#       caption = paste("Variance inflation factors",lab),col.names = c("vif"))
```


### Censored Data 

All COCs  had non-detect (left-censored) data present. Ecology flagged non-detect data and provided the reporting limit for each non-detect value. For purposes of model selection, non-detect values were substituted with the reporting limit. For regression, concentration was modeled as a two-parameter response variable according to the approaches detailed by Hadfield [-@hadfield2010mcmc] and Helsel [-@Helsel2012]. We set the first parameter equal to the measured concentration and the second parameter equal to the reporting limit. We then specified a censored-gaussian prior distribution or censored-log-gaussian prior distribution.  

### Model Selection 
Four potential models were evaluated for each COC:     
    **1. linear** - Non-transformed concentrations with location random effects    
    **2. log-linear** - Log-transformed concentrations with location random effects    
    **3. linear seasonal** - Non-transformed concentrations with location random effects with the addition of a seasonal fixed-effect    
    **4. log-linear seasonal** - Log-transformed concentrations with location random effects with the addition of a seasonal fixed-effect    

Seasonal factors were included in the model selection by designating an integer corresponding to the season in which data were collected: *(1 = Winter, 2 = Spring, 3 = Summer, 4 = Autumn)*

Model selection was performed through Backward elimination of random-effect terms followed by backward elimination of fixed-effect terms. Denominator degrees of freedom and F-statistics were calculated using Satterthwaite's method. 

For each model we calculated the Akaike information criterion (AIC) estimator. The model with the lowest AIC was selected for regression. 


<!-- ```{r qqplots, echo=FALSE} -->

<!-- par(mfrow=c(2,5)) -->
<!-- show_resultsFun(1) -->
<!-- show_resultsFun(2) -->
<!-- show_resultsFun(3) -->
<!-- show_resultsFun(4) -->
<!-- show_resultsFun(5) -->
<!-- show_resultsFun(6) -->
<!-- show_resultsFun(7) -->
<!-- show_resultsFun(8) -->
<!-- show_resultsFun(9) -->
<!-- show_resultsFun(10) -->
<!-- ``` -->



<!-- <!-- ```{r} --> 
<!-- j=1 -->
<!-- models <- results[[j]] -->
<!-- summary(models[[1]]) -->
<!-- for (k in 2:2){ -->
<!--   print((summary(models[[k]]))) -->
<!--   #plot(models[[k]],,main=paste(lab,"\n","Resididuals")) -->

<!-- } -->
<!-- ``` -->


```{r}

tableSummary <- function(z){
coc = params[z]
tab_model(results[coc][[1]],   show.aic = TRUE,auto.label = FALSE, title=coc, dv.labels = c('linear','log-linear','linear seasonal','log-linear seasonal'))
}

tableSummaryNoSeason <- function(z){
coc = params[z]
tab_model(c(results[coc][[1]][1],results[coc][[1]][2]),   show.aic = TRUE,auto.label = FALSE, title=paste0(coc, " - No seasonal efects"), dv.labels = c('linear','log-linear'))
}

tableSummaryYesSeason <- function(z){
coc = params[z]
tab_model(c(results[coc][[1]][3],results[coc][[1]][4]),   show.aic = TRUE,auto.label = FALSE, title=paste0(coc, " - With seasonal efects"), dv.labels = c('linear seasonal','log-linear seasonal'))
}

```



Linear model results for each COC are shown in the following tables. In general, the model with the lowest AIC was selected to move on to the Bayesian analysis. Only predictors that are statistically significant are shown. In some cases, no predictors were significant. In those cases, only the model intercept is reported. 

#### Zinc

**Model with lowest AIC:** `log-linear`    
**Predictors Selected:**    `change_year_index`

```{r Zn, fig.cap="Zinc Table",results='asis'}
z=1

if(is_latex_output()){
stargazer(results[params[z]],type="latex",header=FALSE,title = params[z])
} else{
tableSummaryNoSeason(z)
tableSummaryYesSeason(z)
}
```

#### Copper

**Model with lowest AIC:** `log-linear`    
**Predictors Selected:**    `logtraffic`,`change_year_index`
```{r,results='asis'}
z=2

if(is_latex_output()){
stargazer(results[params[z]],type="latex",header=FALSE,title = params[z])} else{
tableSummaryNoSeason(z)
tableSummaryYesSeason(z)
}

```
#### Nitrite-Nitrate

**Model with lowest AIC:** `log-linear seasonal` 
**Predictors Selected:**    `LU_ratio`
```{r,results='asis'}
z=3

if(is_latex_output()){
stargazer(results[params[z]],type="latex",header=FALSE,title = params[z])} else{
tableSummaryNoSeason(z)
tableSummaryYesSeason(z)
}
```
#### Lead

**Model with lowest AIC:** `log-linear` *(seasonal effects not significant at model selection)*    
**Predictors Selected:**   `change_year_index`, 
```{r,results='asis'}
z=4

if(is_latex_output()){
stargazer(results[params[z]],type="latex",header=FALSE,title = params[z])} else{
tableSummaryNoSeason(z)
tableSummaryYesSeason(z)
}
```

#### Total Phosphorus 

**Model with lowest AIC:** `log-linear seasonal`    
**Predictors Selected:**   `logtraffic`
```{r,results='asis'}
z=5

if(is_latex_output()){
stargazer(results[params[z]],type="latex",header=FALSE,title = params[z])} else{
tableSummaryNoSeason(z)
tableSummaryYesSeason(z)
}

```
#### Total Suspended Solids

**Model with lowest AIC:** `log-linear` *(seasonal effects not significant at model selection)*     
**Predictors Selected:**   `logtraffic`
```{r,results='asis'}
z=6

if(is_latex_output()){
stargazer(results[params[z]],type="latex",header=FALSE,title = params[z])} else{
tableSummaryNoSeason(z)

}

```


#### Total Phthalate
**Model with lowest AIC:** `log-linear`    *(seasonal effects not significant at model selection)*  
**Predictors Selected:**   none
```{r,results='asis'}
z=7

if(is_latex_output()){
stargazer(results[params[z]],type="latex",header=FALSE,title = params[z])} else{
tableSummaryNoSeason(z)
YesSeason(z)
}

```

#### Total PAH
**Model with lowest AIC:** `log-linear`    
**Predictors Selected:**   `LU_ratio`
```{r,results='asis'}
z=8

if(is_latex_output()){
stargazer(results[params[z]],type="latex",header=FALSE,title = params[z])} else{
tableSummaryNoSeason(z)
tableSummaryYesSeason(z)
}


```


#### Total CPAH  *(seasonal effects not significant at model selection)*  
**Model with lowest AIC:** `linear` 
**Predictors Selected:**   `logPopulation`, `logtraffic`
```{r,results='asis'}
z=9

if(is_latex_output()){
stargazer(results[params[z]],type="latex",header=FALSE,title = params[z])} else{
tableSummaryNoSeason(z)
tableSummaryYesSeason(z)
}

```

#### Total HPAH 
**Model with lowest AIC:** `linear`*(seasonal effects not significant at model selection)*   
**Predictors Selected:**   none
```{r,results='asis'}
z=10

if(is_latex_output()){
stargazer(results[params[z]],type="latex",header=FALSE,title = params[z])} else{
tableSummaryNoSeason(z)
tableSummaryYesSeason(z)
}

```

### Bayesian Regression 

We performed censored bayesian regression using a Markov chain Monte Carlo approach. We use the the MCMCglmm package in R [@Helsel2012] for analysis. 

Models were run for 60,000 iterations with a burn-in interval of 10,000 and a thinning interval of 13. For each regression model, we specified either a censored Gaussian or a censored log-Gaussian prior distribution, depending on the model selection results. We specified a simple prior co-variance matrix where covariances between predictors was fixed at 1. We explored other prior variances were not sensitive to these changes.  

```{r}
clean.MCMC <- function(x) {
  sols <-
    summary(x)$solutions ## pull out relevant info from model summary
  Gcovs <- summary(x)$Gcovariances
  Rcovs <- summary(x)$Rcovariances
  fixed <-
    data.frame(row.names(sols), sols, row.names = NULL) ## convert to dataframes with the row.names as the first col
  random <- data.frame(row.names(Gcovs), Gcovs, row.names = NULL)
  residual <- data.frame(row.names(Rcovs), Rcovs, row.names = NULL)
  names(fixed)[names(fixed) == "row.names.sols."] <-
    "variable" ## change the columns names to variable, so they all match
  names(random)[names(random) == "row.names.Gcovs."] <- "variable"
  names(residual)[names(residual) == "row.names.Rcovs."] <-
    "variable"
  fixed$effect <-
    "fixed" ## add ID column for type of effect (fixed, random, residual)
  random$effect <- "random"
  residual$effect <- "residual"
 modelTerms <- as.data.frame(bind_rows(fixed, random, residual))%>%
   dplyr::select(-eff.samp)# merge it all together
} 
```

```{r}
# Perform MCMC modeling 
## Functions 
#Some helper functions to help 

##### Function to add a survival object to the S8 dataframe 
add_surv <- function(df) {
  df$cenMin <- ifelse(df$nondetect_flag,-Inf, (df$concentration))
  df$cenMax  <- (df$concentration)
  df$cenMin_log <- ifelse(df$nondetect_flag,-Inf, log(df$concentration))
  df$cenMax_log  <- log(df$concentration)
  
  return(df)
}
```

```{r}
##### Function to return a chart of predictions from the model 
scatter_predict <- function(model_df,predictions) {
# model_to_predict <- CuModel 
# coc = params[2]
# df <- (subset(s8data.wPredictors, parameter == coc)) %>%
#   add_surv()
# predictions <- predict(model_to_predict, newdata=df, 
#          type="response", interval="none", level=0.95, it=NULL, 
#          posterior="all", verbose=FALSE, approx="numerical")
# 
 obs <- log(model_df$concentration)
 
 ggstatsplot::ggscatterstats(
  data =tibble(p = predictions, obs = obs,L = model_df$Location),
  x = p,
  y = obs,
  type = "bf",
  point.width.jitter = 0.02,
  #point.height.jitter = 0.1,
  marginal = FALSE,
  xlab = "Predicted log(µg/L)",
  ylab = "Observed  log(µg/L)",
  title = coc,
  results.subtitle = FALSE,
  subtitle = "Predictions vs. Observations",
  smooth.line.args = list(size = 1, color = "blue"),
  messages = FALSE
)
}

```

```{r}
##### add tidy and glance functions to the mcmcglmm objects since they don't play nicely with tidyverse objects. 

# add custom functions to extract estimates (tidy) and goodness-of-fit (glance) information
tidy.MCMCglmm <- function(object, ...) {
    s <- summary(object, ...)
    ret <- tibble::tibble(term = row.names(s$solutions),
                          estimate = s$solutions[, 1],
                          conf.low = s$solutions[, 2],
                          conf.high = s$solutions[, 3])
    ret
}
glance.MCMCglmm <- function(object, ...) {
    ret <- tibble::tibble(dic = object$DIC,
                          n = nrow(object$X))
    ret
}

# estimate a simple model
#model <- MCMCglmm(PO ~ 1 + plate, random = ~ FSfamily, data = PlodiaPO, verbose=FALSE, pr=TRUE)
```


```{r mcmccalcFunction}

## mcmc_calc function  

# This is the main funciton to run the mcmc model. It does the following:   
# 1. subsets a dataframe to include only the parameter we want to predict  
# 2. adds a survival object to handle censored data  
# 3. sets up a simiple prior structure  
# 4. performs mcmc modeling on either the log-transformed or non-log transformed responses.   
# 5. returns the results   
mcmc_calc <- function(coc.local, fixed_list, lhs) {
  
  df <- (subset(s8data.wPredictors, parameter == coc.local))

   data <-
  df %>%
  add_surv()
#make the prior_structures 
prior.1<-list(R=list(V=1, fix=1), G=list(G1=list(V=1, nu=0.002)))
prior.2<-list(R=list(V=2, fix=1), G=list(G1=list(V=1, nu=0)))

  
  if (lhs == 'log') {
    mcmc_formula <-
      as.formula(paste(
        "cbind(cenMin_log, cenMax_log) ~ ",
        paste0(fixed_list , collapse = "+")
      ))
  }
  else {
    mcmc_formula <-
      as.formula(paste(
        "cbind(cenMin, cenMax) ~ ",
        paste0(fixed_list , collapse = "+")
      ))
  }
  
  mcmc_results <-
    MCMCglmm(
      mcmc_formula,
      random = ~ Location,
      data =  data,
      family = "cengaussian", 
      verbose = FALSE, prior = prior.1, singular.ok = TRUE,
      nitt = 60000, thin = 13, burnin = 10000
    )
    return((mcmc_results))
}
```



```{r}
# Do some predictive checks 
# ##### function that returns bayesian plots
library(bayesplot)
color_scheme_set("blue")
bayes_plots <- function(fit,coc,df) {
#fit <- TSSModel

#coc = 'Total Suspended Solids - Water - Total'

#df <- (subset(s8data.wPredictors, parameter == coc)) %>%
 # add_surv()
yrep_c <- predict(fit, newdata=df, 
         type="response", interval="confidence", level=0.9, it=NULL, 
         posterior="all", verbose=FALSE, approx="numerical")
yrep_p <- predict(fit, newdata=df, 
         type="response", interval="prediction", level=0.9, it=NULL, 
         posterior="all", verbose=FALSE, approx="numerical")

#show uncertainty intervals under esimated posterior density curves 
plot.1 <- mcmc_areas(fit$Sol,prob = 0.80, pprob_outer = 0.95,point_est="mean")+ggplot2::labs(title = coc, subtitle   = "Posterior distributions with medians and 80% intervals")

#generate scatter plot of predictions 

colnames(yrep_p) <-  c("fit.p", "lwr.p", "upr.p")
scatterdata <- cbind(df, yrep_c, yrep_p)

#generate scatter plot of predictions 
plot.2 <- ggplot(scatterdata) + 
  geom_ribbon(aes(ymin = lwr.p, ymax = upr.p, x = fit),fill="grey", alpha = 0.5) + 
  geom_ribbon(aes(ymin = lwr, ymax = upr, x = fit), fill = "grey", alpha = 0.8) + 
  geom_line(aes(x=fit,y=fit),color="blue",linetype=5)+
  geom_point(aes(x = fit, y = log(concentration)), alpha = 0.5)+
  ggplot2::labs(x="yrep",y="fit",title = coc, subtitle   = "Simulated vs observed values.",caption="dark shade: confidence intervals about posterior mean \n light shade: prediction intervals")


#simulate with 100 draws  
ysim <- (simulate(fit,nsim = 100))


#overlay of predictions 
plot.3<- ppc_dens_overlay(log(df$concentration),t(ysim))+ggplot2::labs(x="log concentration, μg/L ",title = coc, subtitle   = "Observed (y) vs. simulated draws (yrep)")


return(list(plot.1,plot.2,plot.3))
}
```

```{r}
#function for other diagnostic plots 
diagnostic_plots <- function(chains,coc) {
  plotTrace(chains,axes=TRUE,same.limits=TRUE)
  plotDens(chains,main=paste('Posterior Distributions \n',coc),probs=c(0.050,0.950),same.limits=FALSE)}
```


## Results 



### Zinc   
#### Regression Coefficients 
Regression results for Total Zinc are summarized in Table \@ref(tab:znResults).
```{r znResults}
# 
coc <- params[1]
ZnModel <- mcmc_calc(coc,c('change_year_index'),'log')
mod <- ZnModel
df <- (subset(s8data.wPredictors, parameter == coc)) %>%
  add_surv()

#---
ZnPlots <- bayes_plots(mod,coc,df)
kable(clean.MCMC(mod),caption = paste0(coc, " - Bayseian Regression Results "),
      col.names = c("Predictor", "Posterior Mean", "lower 95% CI", "upper 95% CI",  "MCMC p value", "effect"))
```

#### Posterior Uncertainty Levels
Estimated posterior density curves with 80% confidence intervals are shown in Figure \@ref(fig:znPDC).
```{r znPDC}
ZnPlots[1]
```
    
#### Model Diagnostics    
Figure \@ref(fig:zntrace) shows the diagnostic trace plot of MCMC draws. This was used to verify prior distribution assumptions.  
```{r zntrace}
mcmc_trace(mod$Sol)
```


### Copper
#### Regression Coefficients 
Regression results for Total Zinc are summarized in Table \@ref(tab:curesults).
```{r curesults, fig.height=7}
coc = 'Copper - Water - Total'
CuModel <- mcmc_calc(coc,c(
'logtraffic','change_year_index'),'log')
mod <- CuModel


df <- (subset(s8data.wPredictors, parameter == coc)) %>%
  add_surv()
#---
cuPlots <- bayes_plots(mod,coc,df)
kable(clean.MCMC(mod),caption = paste0(coc, " - Bayseian Regression Results "),
      col.names = c("Predictor", "Posterior Mean", "lower 95% CI", "upper 95% CI",  "MCMC p value", "effect"))

```
#### Posterior Uncertainty Levels
Estimated posterior density curves with 80% confidence intervals are shown in Figure \@ref(fig:cuPDC).

```{r cuPDC}
cuPlots[1]
```
    
#### Model Diagnostics    
Figure \@ref(fig:cutrace) shows the diagnostic trace plot of MCMC draws. This was used to verify prior distribution assumptions.  
```{r cutrace}
mcmc_trace(mod$Sol)
```


### Nitrite-Nitrate
#### Regression Coefficients 
Regression results for Nitrite-Nitrate are summarized in Table \@ref(tab:nnResults).
```{r nnResults}
coc = 'Nitrite-Nitrate - Water - Dissolved'
NN_model <- mcmc_calc(coc,c(
'LU_ratio'),'log')
mod <- NN_model

df <- (subset(s8data.wPredictors, parameter == coc)) %>%
  add_surv()
#---
nnPlots <- bayes_plots(mod,coc,df)
kable(clean.MCMC(mod),caption = paste0(coc, " - Bayseian Regression Results "),
      col.names = c("Predictor", "Posterior Mean", "lower 95% CI", "upper 95% CI",  "MCMC p value", "effect"))

```

#### Posterior Uncertainty Levels
Estimated posterior density curves with 80% confidence intervals are shown in Figure \@ref(fig:nnPDC).
```{r nnPDC}
nnPlots[1]
```
    
#### Model Diagnostics    
Figure \@ref(fig:nntrace) shows the diagnostic trace plot of MCMC draws. This was used to verify prior distribution assumptions.  
```{r nntrace}
mcmc_trace(mod$Sol)
```

### Lead   
#### Regression Coefficients 
Regression results for Total Lead are summarized in Table \@ref(tab:pbResults).
```{r pbResults, fig.height=7}
coc <- params[4]
PbModel <- mcmc_calc(params[4],c('impervious'),'log')

mod <- PbModel

df <- (subset(s8data.wPredictors, parameter == coc)) %>%
  add_surv()
#---
pbPlots <- bayes_plots(mod,coc,df)
kable(clean.MCMC(mod),caption = paste0(coc, " - Bayseian Regression Results "),
      col.names = c("Predictor", "Posterior Mean", "lower 95% CI", "upper 95% CI",  "MCMC p value", "effect"))


```
#### Posterior Uncertainty Levels
Estimated posterior density curves with 80% confidence intervals are shown in Figure \@ref(fig:pbPDC).
```{r pbPDC}
pbPlots[1]
```
#### Model Diagnostics
Figure \@ref(fig:pbtrace) shows the diagnostic trace plot of MCMC draws. This was used to verify prior distribution assumptions.  
```{r pbtrace}
mcmc_trace(mod$Sol)
```


### Total Phosphorus
#### Regression Coefficients
Regression results for Total Phosphorus are summarized in Table \@ref(tab:tpResults).
```{r tpResults, fig.height=7}
coc = 'Total Phosphorus - Water - Total'
TPModel <- mcmc_calc('Total Phosphorus - Water - Total',c('logtraffic'),'log')
mod <- TPModel


df <- (subset(s8data.wPredictors, parameter == coc)) %>%
  add_surv()
#---
tpPlots <- bayes_plots(mod,coc,df)
kable(clean.MCMC(mod),caption = paste0(coc, " - Bayseian Regression Results "),
      col.names = c("Predictor", "Posterior Mean", "lower 95% CI", "upper 95% CI",  "MCMC p value", "effect"))

```
#### Posterior Uncertainty Levels
Estimated posterior density curves with 80% confidence intervals are shown in Figure \@ref(fig:tpPDC).
```{r tpPDC}
tpPlots[1]

```
    
#### Model Diagnostics    
Figure \@ref(fig:tptrace) shows the diagnostic trace plot of MCMC draws. This was used to verify prior distribution assumptions.  
```{r tptrace}
mcmc_trace(mod$Sol)
```
### Total Suspended Solids 
#### Regression Coefficients 
Regression results for Total Suspended Solids  are summarized in Table \@ref(tab:tssResults). 

```{r tssResults}
coc = 'Total Suspended Solids - Water - Total'
TSSModel <- mcmc_calc('Total Suspended Solids - Water - Total','logtraffic','log')
mod <- TSSModel
df <- (subset(s8data.wPredictors, parameter == coc)) %>%
  add_surv()
#---
tssPlots <- bayes_plots(mod,coc,df)
kable(clean.MCMC(mod),caption = paste0(coc, " - Bayseian Regression Results "),
      col.names = c("Predictor", "Posterior Mean", "lower 95% CI", "upper 95% CI",  "MCMC p value", "effect"))


```
#### Posterior Uncertainty Levels
Estimated posterior density curves with 80% confidence intervals are shown in Figure \@ref(fig:tssPDC).
```{r tssPDC}
tssPlots[1]
```
    
#### Model Diagnostics    
Figure \@ref(fig:tsstrace) shows the diagnostic trace plot of MCMC draws. This was used to verify prior distribution assumptions.  
```{r tsstrace}
mcmc_trace(mod$Sol)
```

### Total PAH 
#### Regression Coefficients 
Regression results for Total PAH  are summarized in Table \@ref(tab:pahResults). 
```{r pahResults}
coc = 'Total PAH - Water - Total'
PAHModel<- mcmc_calc('Total PAH - Water - Total','LU_ratio','log') 
mod <- PAHModel
df <- (subset(s8data.wPredictors, parameter == coc)) %>%
  add_surv()
#---
pahPlots <- bayes_plots(mod,coc,df)
kable(clean.MCMC(mod),caption = paste0(coc, " - Bayseian Regression Results "),
      col.names = c("Predictor", "Posterior Mean", "lower 95% CI", "upper 95% CI",  "MCMC p value", "effect"))
```
#### Posterior Uncertainty Levels
Estimated posterior density curves with 80% confidence intervals are shown in Figure \@ref(fig:pahPDC).
```{r pahPDC}
pahPlots[1]
```
    
#### Model Diagnostics    
Figure \@ref(fig:pahtrace) shows the diagnostic trace plot of MCMC draws. This was used to verify prior distribution assumptions.  
```{r pahtrace}
mcmc_trace(mod$Sol)
```

### Total CPAH     
#### Regression Coefficients     
Regression results for Total CPAH  are summarized in Table \@ref(tab:cpahResults). 
```{r cpahResults}
coc = 'CPAH - Water - Total'
CPAHModel<- mcmc_calc('CPAH - Water - Total',c('logPopulation','logtraffic'),'log') 
mod <- CPAHModel
df <- (subset(s8data.wPredictors, parameter == coc)) %>%
  add_surv()
#---
cpahPlots <- bayes_plots(mod,coc,df)
kable(clean.MCMC(mod),caption = paste0(coc, " - Bayseian Regression Results "),
      col.names = c("Predictor", "Posterior Mean", "lower 95% CI", "upper 95% CI",  "MCMC p value", "effect"))
```
#### Posterior Uncertainty Levels
Estimated posterior density curves with 80% confidence intervals are shown in Figure \@ref(fig:cpahPDC).
```{r cpahPDC}
cpahPlots[1]
```
    
#### Model Diagnostics    
Figure \@ref(fig:cpahtrace) shows the diagnostic trace plot of MCMC draws. This was used to verify prior distribution assumptions.  
```{r cpahtrace}
mcmc_trace(mod$Sol)
```

### Total HPAH            
#### Regression Coefficients          
```{r}
coc = 'HPAH - Water - Total'
HPAHModel<- mcmc_calc('HPAH - Water - Total',c('LU_ratio'),'log') 
mod <- HPAHModel
df <- (subset(s8data.wPredictors, parameter == coc)) %>%
  add_surv()
#---
hpahPlots <- bayes_plots(mod,coc,df)
kable(clean.MCMC(mod),caption = paste0(coc, " - Bayseian Regression Results "),
      col.names = c("Predictor", "Posterior Mean", "lower 95% CI", "upper 95% CI",  "MCMC p value", "effect"))
```
#### Posterior Uncertainty Levels
Estimated posterior density curves with 80% confidence intervals are shown in Figure \@ref(fig:hpahPDC).
```{r hpahPDC}
hpahPlots[1]
```
    
#### Model Diagnostics    
Figure \@ref(fig:hpahtrace) shows the diagnostic trace plot of MCMC draws. This was used to verify prior distribution assumptions.  
```{r hpahtrace}
mcmc_trace(mod$Sol)
```

## Predictions        

We used the models developed above to generate predictions of pollutant concentrations to verify regression parameters. We first evaluated the posterior means of the predictors to evaluate data fit. We then simulated 100 draws from the full posterior distributions to evaluate the range of predictions. Results are show below. 



```{r echo=FALSE}
ZnPlots[2]
cuPlots[2]
nnPlots[2]
pbPlots[2]
tpPlots[2]
tssPlots[2]
pahPlots[2]
cpahPlots[2]
hpahPlots[2]
###Nitrite-Nitrate
###Lead - Water - Total
###Total Phosphorus - Water - Total
###Total Suspended Solids - Water - Total
###Total Phthalate - Water - Total
###Total 
###CPAH 
###HPAH 
```

## Discussion 

Our  models appear to fit both the mean concentrations within each location and capture the full spread of the data within the bounds of the 95% confidence intervals.  Model fit appears to be the strongest for metals (Zinc, Copper, Lead) and not as strong for organics (PAH, CPAH and HPAH). These may be improved through specifying a better prior distribution.  

To our knowledge, this work represents the first effort to predict stormwater pollution loading from landscape characteristics, rather than from simple land use categories. In the stormwaterheatmap tool, we use these predictive models to spatially output predicted levels of commonly reported stormwater pollutants. 

## Generation of heatmap layers 

To produce heatmap layers of pollutant concentration, we use the original spatial predictor layers along with a linear relationship between response and predictors. To avoid extrapolating beyond the available data, we first clamp the predictors to the high and low values observed with monitored watersheds. We then reduce predictor layers into an array-valued image, where each pixel in the model domain contains an array of predictor values. We then multiply this layer by an array that contains the intercept of the regression relationship, along with coefficients. 

For example for each COC, we have a linear regression in the form of:  

$$
y_{i,j\ }=\ \beta_{0\ }+\beta_1x_1+\ ...\ +\beta_{n\ }+\epsilon_{i,j}
$$

Where β~0~ is the intercept and B~n~ are the regression coefficients. We express this as a 1-D array in the form of 
$$
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
...\\\
\beta_n
\end{bmatrix}
$$

The predictor layer is comprised of pixels with 1-D arrays of predictor values in the form of: 
$$
\begin{bmatrix}
1 \\
x_1 \\
x_2 \\
...\\\
x_n
\end{bmatrix}
$$

We then calculate the dot product of the two arrays and reduce the values to a single image. This is expressed in the same manner as the linear regression equation: 

$$


y = \beta_0+\beta_1x_1+...\beta_nx_n 

$$
 
<!-- References at the end here.  -->
<div id = "refs"></div>

```{r}

```

